{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/punygod_admin/SoundSense/soundsense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punygod_admin/miniconda3/envs/muls/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-26 14:36:10.899968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.baselines.mulsa.inference import MULSAInference\n",
    "from models.imi_datasets import ImitationEpisode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/punygod_admin/SoundSense/soundsense/models/baselines/mulsa/lightning_logs/sorting_imi_vg_ag_lstm_seqlen_3_spec04-22-21:39:20/last.ckpt\"\n",
    "config_path = '/home/punygod_admin/SoundSense/soundsense/models/baselines/mulsa/lightning_logs/sorting_imi_vg_ag_lstm_seqlen_3_spec04-22-21:39:20/hparams.yaml'\n",
    "with open(config_path) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punygod_admin/miniconda3/envs/muls/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/home/punygod_admin/miniconda3/envs/muls/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/home/punygod_admin/miniconda3/envs/muls/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/home/punygod_admin/miniconda3/envs/muls/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Creating LSTM decoder\n",
      "Total parameters: 29.449992 Million\n",
      "NUM STACK:  6\n"
     ]
    }
   ],
   "source": [
    "model = MULSAInference(config_path)\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model.load_state_dict(\n",
    "        torch.load(\n",
    "            model_path,\n",
    "            map_location=torch.device(\"cpu\"),\n",
    "        )['state_dict']\n",
    "    )\n",
    "# model = model.load_from_checkpoint(model_path)\n",
    "\n",
    "# Then, move the model to CUDA device\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train episodes:  79\n",
      "Val episodes:  20\n",
      "Loading audio for ng/87\n",
      "Loading audio for ng/94\n",
      "Loading audio for ng/42\n",
      "Loading audio for ng/98\n",
      "Loading audio for ng/86\n",
      "Loading audio for ng/80\n",
      "Loading audio for ng/48\n",
      "Loading audio for ng/16\n",
      "Loading audio for ing/4\n",
      "Loading audio for ng/97\n",
      "Loading audio for ng/71\n",
      "Loading audio for ng/78\n",
      "Loading audio for ng/17\n",
      "Loading audio for ng/83\n",
      "Loading audio for ing/7\n",
      "Loading audio for ng/69\n",
      "Loading audio for ng/19\n",
      "Loading audio for ng/56\n",
      "Loading audio for ing/9\n",
      "Loading audio for ng/88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(0)\n",
    "run_ids = os.listdir('/home/punygod_admin/SoundSense/soundsense/data/mulsa/sorting')\n",
    "np.random.permutation(run_ids)\n",
    "train_val_split = 0.8\n",
    "split = int(train_val_split*len(run_ids))\n",
    "train_episodes = run_ids[:split]\n",
    "val_episodes = run_ids[split:]\n",
    "\n",
    "print(\"Train episodes: \", len(train_episodes))\n",
    "print(\"Val episodes: \", len(val_episodes))\n",
    "val_set = torch.utils.data.ConcatDataset(\n",
    "    [\n",
    "        ImitationEpisode(config, run_id, train=False)\n",
    "        for run_id in val_episodes\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(val_set, num_workers=config[\"num_workers\"], shuffle=False,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/punygod_admin/miniconda3/envs/muls/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/home/punygod_admin/miniconda3/envs/muls/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 501])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m img[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m img[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(img[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 5\u001b[0m inp \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(img[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: img[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[43mdevice\u001b[49m)}  \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m model(inp)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "for i, (img, target) in enumerate(val_loader):\n",
    "    img[0] = img[0].squeeze(0)\n",
    "    img[0] = img[0].permute(0, 2, 3, 1)\n",
    "    print(img[1].squeeze(0).shape)\n",
    "    inp = {\"video\": np.array(img[0]), \"audio\": img[1].squeeze(0).to(device)}  \n",
    "\n",
    "    # Forward pass through the model\n",
    "    output = model(inp)\n",
    "    output = output[0][-1].reshape(1,-1)\n",
    "    print(output.data.shape)\n",
    "    _, predicted_class = torch.max(output.data, 1)\n",
    "\n",
    "    if predicted_class != 0:\n",
    "        continue\n",
    "\n",
    "    # Compute the gradients of the predicted class output with respect to the feature maps\n",
    "    model.zero_grad()\n",
    "    output[:, predicted_class].backward()\n",
    "\n",
    "\n",
    "    # Get the gradients from the last convolutional layer\n",
    "    v_grads, a_grads = model.get_activations_gradient()\n",
    "    print(\"a_grads: \", a_grads.shape) # torch.Size([6, 512, 5, 7])\n",
    "\n",
    "    # Get the feature maps from the last convolutional layer\n",
    "    v_maps, a_maps = model.get_activations()\n",
    "    print(\"a_maps: \", a_maps.shape) # torch.Size([6, 512, 5, 7])\n",
    "\n",
    "    # Perform global average pooling on the gradients\n",
    "    a_pooled_grads = torch.mean(a_grads, dim=(2, 3))\n",
    "    print(\"a_pooled_grads: \", a_pooled_grads.shape) # torch.Size([6, 512])\n",
    "\n",
    "    # Multiply each feature map by its corresponding gradient value\n",
    "    for i in range(a_maps.shape[1]):\n",
    "        for j in range(a_maps.shape[0]):\n",
    "            a_maps[j, i, :, :] *= a_pooled_grads[j, i]\n",
    "    print(\"v_maps: \", a_maps.shape) # torch.Size([6, 512, 5, 7])\n",
    "    \n",
    "    # Obtain the heatmap by averaging the weighted feature maps\n",
    "    a_heatmap = torch.mean(a_maps, dim=1).squeeze().cpu()\n",
    "    print(\"v_heatmap: \", a_heatmap.shape) # torch.Size([6, 5, 7])\n",
    "\n",
    "    # Normalize the heatmap\n",
    "    a_heatmap = torch.maximum(a_heatmap, torch.zeros_like(a_heatmap))\n",
    "    print(\"a_heatmap: \", a_heatmap.shape) # torch.Size([6, 5, 7])\n",
    "    # print(type(v_heatmap)) # <class 'torch.Tensor'>\n",
    "    # print(torch.max(torch.max(v_heatmap, dim=1)[0], dim=1)[0]) \n",
    "\n",
    "    a_heatmap_max = torch.max(torch.max(a_heatmap, dim=1)[0], dim=1)[0] # torch.Size([6])\n",
    "    for i in range(a_heatmap.shape[0]):\n",
    "        a_heatmap[i] /= a_heatmap_max[i]\n",
    "\n",
    "    print(\"v_heatmap: \", a_heatmap.shape) # torch.Size([6, 5, 7])\n",
    "    \n",
    "    # Resize the heatmap to match the input image size\n",
    "    img[0] = img[0].permute(0, 3, 1, 2)\n",
    "    print(\"img[0].shape: \", img[0].shape) # torch.Size([6, 3, 75, 100])\n",
    "\n",
    "\n",
    "    a_heatmap = a_heatmap.numpy()\n",
    "    a_heatmap_resized = np.zeros((a_heatmap.shape[0], img[0].shape[2], img[0].shape[3])) # (6, 75, 100)\n",
    "\n",
    "    for i in range(a_heatmap.shape[0]):\n",
    "        a_heatmap_resized[i] = cv2.resize(a_heatmap[i], (img[0].shape[3], img[0].shape[2])) # (75, 100)\n",
    "\n",
    "    a_heatmap_resized = (255 * a_heatmap_resized).astype(np.uint8) \n",
    "    a_heatmap_resized = np.minimum(a_heatmap_resized, 255)\n",
    "    print(\"a_heatmap_resized: \", a_heatmap_resized.shape) # (6, 75, 100)   \n",
    "\n",
    "    # Apply the heatmap to the input image\n",
    "    superimposed_img_a = np.zeros((a_heatmap.shape[0], img[0].shape[2], img[0].shape[3], 3))\n",
    "    for i in range(a_heatmap.shape[0]):\n",
    "        temp_heatmap = cv2.applyColorMap(a_heatmap_resized[i], cv2.COLORMAP_JET) / 255\n",
    "        # print(min(temp_heatmap.flatten()), max(temp_heatmap.flatten())) # 0 1\n",
    "        temp_img = img[0][i].cpu().numpy().transpose(1, 2, 0)*0.5 + 0.5\n",
    "        # print(min(temp_img.flatten()), max(temp_img.flatten())) # 0 1\n",
    "        superimposed_img_a[i] = temp_heatmap * 0.3 + temp_img * 0.7\n",
    "\n",
    "    # Display the input image and the GradCAM heatmap with colorbar\n",
    "    fig, axs = plt.subplots(6, 2, figsize=(10, 30))\n",
    "    for i in range(6):\n",
    "        axs[i, 0].imshow(img[0][i].cpu().numpy().transpose(1, 2, 0)*0.5 + 0.5)\n",
    "        fig.colorbar(axs[i, 1].imshow(superimposed_img_a[i]), ax=axs[i, 1], cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_baselines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
